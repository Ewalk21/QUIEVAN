\documentclass[aps,pra,notitlepage,amsmath,amssymb,letterpaper,12pt]{revtex4-1}
\usepackage{amsthm,enumerate,amsmath}
\usepackage{graphicx}
%  Above uses the Americal Physical Society template for Physical Review A
%  as a reasonable and fully-featured default template

%  Below define helpful commands to set up problem environments easily
\newtheorem{theorem}{Theorem}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{solution}{\begin{proof}[Solution]}{\end{proof}}

% --------------------------------------------------------------
%                   Document Begins Here
% --------------------------------------------------------------

\begin{document}

\title{Final Proofs for MATH450 Real Analysis}
\author{Quinn Gates, Evan Walker}
%\affiliation{Math 450, Schmid College of Science and Technology, Chapman University}
\date{\today}

\maketitle

\underline{\textbf{Problem 1.}}
\begin{theorem}[3.24 Bolzano-Weierstrass]
If a bounded set $S \subseteq \mathbb{R}^n$ contains infinitely many points, then there is at least one point in $\mathbb{R}^n$ which is an accumulation point of $S$.
%Application: Thm. 3.25 Cantor Intersection Theorem
\end{theorem}

\begin{proof}
(Case $n=1$,$\quad \mathbb{R}^{1}$)
Let $S \subset \mathbb{R}^{1}$ and $S$ bounded, $\Rightarrow$ since $S$ is bounded, $S$ lies completely within an interval $[a,b]$. Since $S$ contains infinitely many points,
$$[a,\frac{a+b}{2}] \quad \textrm{ and }  \quad [\frac{a+b}{2},b]$$
may either both contain infinitely many points or only one contains infinitely many points. Now define $[a_{1},b_{1}]$ to be one of the previously stated intervals with infinitely many points. Then
$$[a_{1},\frac{a_{1}+b_{1}}{2}] \quad \textrm{ and }  \quad [\frac{a_{1}+b_{1}}{2},b_{1}]$$
again may either both contain infinitely many points or only one contains infinitely many points. Repeating this process $k$ times, we arrive at an interval $[a_{k},b_{k}]$
such that $$sup(a_{k}) = inf(b_{k}) = x$$ where $x \in S$. We aim to show that $x$ is an accumulation point of $S$. Consider $k$ to be large enough such that $b_{k} - a_{k} < \frac{r}{2}$, $\Rightarrow [a_{k},b_{k}] \subseteq B(x,r)$. Since, by construction, $B(x,r)$ contains infinitely many points, $(B(x,r) - \{x\}) \cap S \ne \emptyset$ and hence $x$ is an accumulation point of $S$.
\newline{}
\newline{}
(Case $n=2$,$\quad \mathbb{R}^{2}$) Let $S \subset \mathbb{R}^{2}$ and $S$ bounded, $\Rightarrow$ since $S$ is bounded, $S$ lies completely within a 2-dimensional interval $J$ defined by the cartesian product of two 1-dimensional intervals.
$$J_{1} = [a,b] \times [a,b]$$
similar to the case for $n=1$ above, we continually divide the interval into halfs, choosing the interval with infinitely many points. Let $I_{1}^{(1)} = [a,b]$, and $I_{2}^{(1)} = [a,b]$, Then let the next division of the interval be $I_{1}^{(2)} = [a,\frac{a+b}{2}] \textrm{ or } [\frac{a+b}{2},b] = [a_{1},b_{1}]$, and $I_{2}^{(2)} = [a,\frac{a+b}{2}] \textrm{ or } [\frac{a+b}{2},b] = [a_{2},b_{2}]$, Again choosing whichever interval has infinitely many points. If they both contain infinitely many points, our choice is arbitrary. Hence, define $J_{n} = I_{1}^{n} \times I_{2}^{n}$. For a large enough $n$, we have that $J_{n}$ has the property that
$$sup(a_{1}^{(n)}) = inf(b_{1}^{(n)}) = x_{1} \quad \textrm{ and } \quad sup(a_{2}^{(n)}) = inf(b_{2}^{(n)}) = x_{2}$$
We assert that $x=(x_{1}, x_{2})$ is an accumulation point of $S$. Note that $b_{k}^{(n)}-a_{k}^{(n)} = \frac{a}{2^{n-2}}$ for $k = 1,2$. take a ball in $\mathbb{R}^{2}$, $B(x,r)$ and let n be large enough such that $\frac{a}{2^{n-2}} < \frac{r}{2}$. Hence $J_{n} \subseteq B(x,r)$, $\Rightarrow$ $B(x,r)$ also contains infinitely many points and we have that,
$$ (B(x,r)- \{x\}) \cap S \ne \emptyset $$
This proves that $x$ is an accumulation point of $S$.
\newline{}
\end{proof}

\textbf{Applications of 3.24}
\begin{theorem}[3.25 The Cantor Intersection Theorem]
Let $\{Q_{1}, Q_{2},...\}$ be a countable collection of nonempty sets in $\mathbb{R}^2$ such that.
\begin{enumerate}[\upshape i)]
\item $Q_{k+1} \subseteq Q_{k} \qquad (k=1,2,3,...)$
\item Each set $Q_{k}$ is closed and $Q_{1}$ is bounded
\end{enumerate}
$\Longrightarrow \quad \cap_{k=1}^{\infty}Q_{k} \ne \emptyset$ and is closed.
\end{theorem}
\begin{proof}
(Case 1: $Q_{m} = Q_{k} \quad \forall m \geq k$)
\newline{}
$\Rightarrow \quad Q_{k+2} \subseteq Q_{k+1} \subseteq Q_{k} \subseteq ... Q_{2} \subseteq Q_{1} \Rightarrow$
$\cap_{n=1}^{\infty} Q_{n} = Q_{k} \ne \emptyset$
\newline{}
(Case 2: $Q_{k+1} \nsubseteq Q_{k}$)
\newline{}
$\Rightarrow$ we can build $A=\{x_{1},x_{2},x_{3}, ... , x_{k},...\}$, an infinite set. where
$x_{k} \in Q_{k}\setminus Q_{k+1} \quad x_{k} \in Q_{k}$. $\Rightarrow \textrm{(by the Bolzano-Weierstrass Theorem)}\quad \exists x \in \mathbb{R}^2 \textrm{  such that } x$ is an accumulation point of A. We want to show
$x \in \cap_{k=1}^{\infty}Q_{k}$. since $x$ is an accumulation point of $A$, $x$ is also an accumulation point for $Q_{k} \quad \forall k$. Since $Q_{k}$ is closed, it contains all of its accumulation points $\Rightarrow x\in Q_{k}$ and therefore
$x \in \cap_{k=1}^{\infty}Q_{k}$.\newline{}
$$ $$
\end{proof}


\underline{\textbf{Problem 2.}}
\begin{theorem}[3.27] \nonumber
Let $G = \{A_{1},A_{2},...\}$ denote the countable collection of all n-balls having rational radii and centers at points with rational coordinates. Assume $x \in \mathbb{R}^n$ and let $S$ be an open set in $\mathbb{R}^n$ and $x \in S$. Then $x \in A_{k} \subseteq S$ for some $k$. That is, $x$ is contained by some n-ball in $G$ which is contained by $S$.
%Application: Thm 3.28 (Lindeloff Covering Theorem)
\end{theorem}
\begin{proof}
Theorem 2.27 states that if G is a countable collection of countable sets, then
\[\cup_{k=1}^{n} A_{k} \qquad A_{k} \in G\]
is also countable. Since $x \in S$, and $S$ is open, we know that there exists an n-ball such that
\[B(x,r) \subseteq S\]
We want to find a rational point $y\in S$ such that $x \in B(y,R) \in G$, or equally stated by $B(y,R) \subseteq B(x,r) \subseteq S$. Let
$x=(x_{1},x_{2},...,x_{n})$ and let $y_{k}$ be a rational number such that
\[\left| y_{k} - x_{k} \right| < \frac{r}{4n} \qquad \textrm{ for each } k=1,2,3,...,n\]
Then
\[\|y-x\| \leq \mid y_{1}-x_{1} \mid + ... + \mid y_{n}-x_{n} \mid < \frac{r}{4}\]
Let us define $R$ as a rational number such that $\frac{r}{4} < R < \frac{r}{2}$. Then
$x \in B(y,R)$ since we found the distance between $y$ and $x$ to be less than $r/4$, so surely $x$ must be contained in $B(y,R)$. By the same reasoning, we have that $B(y,R) \subseteq B(x,r) \subseteq S$. since both $y$ and $R$ are rational by construction, we have that $B(y,R) \in G$. Hence, we've proved the assertion.
\end{proof}
\textbf{Applications of 3.27}
\begin{theorem}[3.28 Lindeloff Covering Theorem]
Assume $A\subseteq \mathbb{R}^n$ and let $f$ be an open covering of $A$. Then: $\exists$ a countable subcollection of $f$ that also covers $A$.  $$A\subseteq \cup_{S\in f'}S$$
\end{theorem}
\begin{proof}
Let $G=\{A_1,A_2, \ldots ,A_k\ldots \}$ countable subset of $B_n(y,q)$ where $y$ and $q$ are both rational numbers.  Then take $x\in A \quad \Rightarrow \quad \exists S \in f$ such that $x\in S \subset \mathbb{R}^n$ Where S is an open subset.  Therefore, we can use Theorem 3.27 to say: $$\exists A_k \in G \textrm{ such that } x\in A_k\subseteq S$$
Define $m=m(x)=min\{k|x\in A_k,A_k\in G\}$  
$$G'=\{A_{m(x)}|x\in A\}  \iff f'=\{S|A_{m(x)}\in S\}$$
Both $G'$ and $f'$ are countable so: $$\Rightarrow \quad A\subseteq \cup_{S\in f'}S$$
\end{proof}

\underline{\textbf{Problem 3}}
\begin{theorem}[3.29 Heine-Borel]
Let $F$ be an open covering of a closed and bounded set $S \subseteq \mathbb{R}^n$. Then a finite subcollection of $F$ also covers $S$.
%Application: you choose
\end{theorem}
\begin{proof}
Assume that \(F\) is an open covering of the set \(S\). A countable subcollection of \(F\), say \{\(I_1,I_2,\ldots\)\} covers \(S\).  Now we consider the finite union:
\[S_m = \bigcup\limits_{k=1}^{m} I_k\]
This set is open since it is the union of open sets.  We shall show that for some value of \(m\) the union \(S_m\) covers \(S\). \\
Consider the complement \(R^n \backslash S_m\), which is closed.  Now define a countable collection of sets \{\(Q_1,Q_2, \ldots\)\} as follows: \(Q_1 = S\) and for \(m>1\),
\[Q_m = S\cap (R^n\backslash S_m)\]
Then \(Q_m\) consists of all those points of \(S\) that are outside of \(S_m\).  Now we just need to show that there is some \(m\) for which \(Q_m\) is empty which will show that for this \(m\) no point of \(S\) lies outside of \(S_m\).
Consider the following: 

    Each set \(Q_m\) is closed since it is the intersection of closed \(S\) and closed \(R^n\backslash S_m\)
    
        The sets \(Q_m\) are decreasing since the \(S_m\) are increasing.  That is: \(Q_{m+1}\subseteq Q_m\). 
        
        The sets \(Q_m\) being subsets of \(S\) are all bounded.
        
        Therefore, if no set \(Q_m\) is empty we can use the \(Cantor Intersection Theorem\) to conclude that the intersection:
\[\bigcap\limits_{k=1}^{\infty} Q_k \neq \emptyset\]

    That means that there is some point in \(S\) which is in all the sets \(Q_m\) or outside all of the sets \(S_m\).  But this is a contradiction since \(S\subseteq \bigcup\limits_{k=1}^{\infty} S_k\).  Therefore some \(Q_m\) must be empty.
\end{proof}
\textbf{Applications of 3.29} \newline
The Heine-Borel Theorem always applies when in a Euclidean space $R^n$, but if an extra condition is added then it will apply in any metric space.  In a metric space \(M=(S,d)\) \(M\) must be both complete and totally bounded.  This means that \(M\) must have the property that every Cauchy sequence in \(M\) converges in \(M\), and it must be possible to cover \(M\) with a finite number of balls with of fixed size.  So having your space be closed and bounded takes care of half but for the theorem to hold in a metric space the hypothesis must also require that the metric space be complete.\newline{}

\underline{\textbf{Problem 4}}
\begin{theorem}[3.31]
Let $S \subseteq \mathbb{R}^n$. Then the following statements are equivalent
\begin{enumerate}[\upshape a)]
  \item $S$ is compact. %note a implies b by heineborel
  \item $S$ is closed and bounded.
  \item Every infinite subset of $S$ has an accumulation point in $S$.
\end{enumerate}
%comparison with metric spaces (without proof)
\end{theorem}
\begin{proof}
To prove equivalence, we will prove that \(a\) implies \(b\),
that \(b\) implies \(c\), that \(c\) implies \(b\), \(b\) implies \(a\) by \(Thm3.29\),
the \(Heine-Borel\) theorem. Once these parts are proven, we will
have equivalence of the statements.

    \(a\implies b\)

    Assume that \(S\) is compact. First we will prove that \(S\) is bounded.
For some \(p\in S\) \(\exists\) \(T\):= a collection of \(n\)-balls:
\(B(p,k), k=1,2,3\ldots\) such that \(T\) is an open covering of the
space \(S\). Since \(S\) is compact, there is a finite subset of that
collection that is still an open covering of \(S\). Therefore, \(S\) is
bounded. :)

    Next we must prove that \(S\) is closed. This will be done through
contradiction so first assume that \(S\) is not closed. Then
\(\exists y \notin S\) where \(y\) is an accumulation point of \(S\).
Now lets say \(\exists x\in S\) and let \(r_x = \frac{\| x-y\|}{2}\)
(half of the distance between x and y) We know that each radius \(r_x\)
is positive since \(y\notin S\), and the collection fo balls
\(B(x,r_x)\) is an open covering of \(S\). Since we know that \(S\) is
compact, there is a finite number, \(n\) of these balls that still
covers \(S\) which is basically the union of all of the balls.
\[S\subset \bigcup\limits_{k=1}^{n} B(x_k,r_k)\] We have a finite number
of radii since we have a finite number of balls so lets say \(r\) is the
smallest radius. Now we want to prove that \(B(y,r)\) has no points in
common with any of the other balls that cover
\(S\left(B\left(x_k,r_k\right)\right)\) So if we assume \(x\in B(y,r)\)
then \(\| x-y\| <r<r_k\). So by the triangle inequality, we have:
\[\| y-x_k\|\leq\| y-x\| +\| x-x_k\| \]
\[\Rightarrow \| x-x_k\|\geq\| y-x_k\| -\| x-y\| = 2r_k-\| x-y\| >r_k\]
\[\Rightarrow x\notin B(x_k,r_k)\Rightarrow B(y,r)\cap S = \emptyset\]
\(\Rightarrow\Leftarrow\) Contradicts the fact that \(y\) is an
accumulation point This shows that \(S\) is therefore closed and
\(a\implies b\) \(\checkmark\)

    \(b\implies a\) by the \(Heine-Borel Theorem\)
    
    Proved in number 3

    \(b\implies c\)

    Assume that \(b\) holds. This will be used to prove \(c\): Every
infinite subset of \(S\) has an accumulation point in \(S\). If \(T\) is
an infinite subset of \(S\), since S is bounded, \(T\) must also be
bounded because \(T\) is a subset of \(S\). By the
\(Bolzano Weierstrass Theorem\), if a bounded set in \(R^n\) contains
infinitely many points, then there is at least one point in \(R^n\)
which is an accumulation point of \(S\). Lets name that accumulation in
\(T\), \(x\). Since \(S\) is closed, we know that the accumulation point
of \(T\) is also the accumulation point of \(S\).
\(\Rightarrow b\implies c\) \(\checkmark\)

    \(c\implies b\)

    Assume that \(c\) holds. \(\rightarrow\) every infinite subset of \(S\)
has an accumulation point in \(S\). We want to prove \(b\) which says
that \(S\) is closed and bounded. First, we are going to say that \(S\)
is unbounded, and prove by contradiction. So assume that \(S\) is
unbounded \(\Rightarrow \forall m>0\exists x_m\in S\) such that
\(\| x_n\| >m\) There is a collection of points \(T:=\)
\{\(x_1,x_2,\ldots\)\} that is an infinite subset of \(S\), and since we
are assuming \((c)\), \(T\) has an accumulation point \(y\in S\). For
\(m>1+\| y\|\) we have: \[\| x_m-y\| \geq\| x_m\| -\|y \| >m-\|y \| >1\]
This contradicts the fact that \(y\) is an accumulation point of \(T\).
Therefore, \(S\) is bounded. :)

    Next we have to prove that \(S\) is closed. Let \(x\) be an accumulation
point of \(S\). Since every set containing a ball with center \(x\)
contains infinitely many points of \(S\), we can consider
\(B(x,\frac{1}{k})\), where \(k = 1,2,3,\ldots\), and get a countable
set of distinct points, \(T:=\) \{\(x_1,x_2,\ldots\)\}\(\subset S\),
such that \(x_k\in B(x,\frac{1}{k})\) The point \(x\) is also an
accumulation point of \(T\) since \(T\subset S\) Since \(T\) is an
infinite subset of \(S\), \((c)\) tells us that \(T\) must have an
accumulation point in \(S\) Next, we have to prove that \(x\) is the
only accumulation point of \(T\). Suppose that \(y\ne x\). By the
triangle inequality we have:
\[\| y-x\| \leq\| y-x_k\| +\|x_k-x\| <\| y-x_k\| +\frac{1}{k} \mid x_k\in T\]
If \(k_0\) is taken to be $k_{0} = \lfloor \frac{2}{\|y-x\|} \rfloor \Rightarrow$ \(\frac{1}{k}<\frac{1}{2}\| y-x\|\)
whenever \(k\geq k_0\), the last inequality leads to
\(\frac{1}{2}\| y-x\| <\| y-x_k\|\) \(\implies x_k\notin B(y,r)\) when
\(k\geq k_0\) if \(r=\frac{1}{2}\| y-x\|\) \(\implies y\) is not an
accumulation point of \(T\). \(\checkmark\)
\(QED\)
\newline{}
Theorem 3.31 does not hold in a general metric space.  This is because the requirements for a metric space to be compact are more rigorous than the requirements for a euclidean space.  In a metric space \(M=(S,d)\) \(M\) must be both complete and totally bounded.  This means that \(M\) must have the property that every Cauchy sequence in \(M\) converges in \(M\), and it must be possible to cover \(M\) with a finite number of balls with of fixed size.  For this reason, we still have that \(a\implies b\) but the theorem will fail when we try to prove that \(b\implies a\).  
    
    Consider the metric space \(R^2\backslash (0,0)\) with the usual metric from \(R^2\).  The set 
    \[D=\{(x,y)\mid 0<x^2+y^2\leq 1\}\]
    is closed and bounded since it is the interior of the unit circle.  But \(D\) is not compact since it will "want" to converge to \((0,0)\) but \((0,0)\) is not an element of the metric so it is not compact hence \(b\) does not imply \(a\).  Therefore, Theorem 3.31 does not apply to the general metric space.  
\end{proof}


\underline{\textbf{The next 3 theorems are with respect to problem 5.}}
\begin{theorem}[theorem 4.16]
Let $f:S \to T$  be a function from one metric space $(S,d_{S})$ to another $(T,d_{T})$, and assume $p \in S$. Then $f$ is continuous at $p$ iff for every sequence $\{x_{n}\}$ in $S$ convergent to $p$, the sequence $\{f(x_{n})\}$ in $T$ converges to $f(p)$. Symbolically,
\[\lim_{n \to \infty} f(x_{n}) = f(\lim_{n \to \infty} x_{n})\]
%Continuous Functions in Metric Spaces: Thm. 4.16, Thm. 4.23, Thm. 4.25
\end{theorem}
\begin{proof}
($\Longrightarrow$) Assume that $f$ is continuous at $p$. The definition of continuity at a point states that for every $\epsilon > 0$ there is a $\delta >0$ such that,
\[d_{T}(f(x),f(p)) < \epsilon \qquad \textrm{  whenever  } d_{S}(x,p) < \delta\]
By Definition 4.1(def convergence) if the sequence $\{x_{n}\} \rightarrow p$ in $S$, then  $\forall \delta > 0 \quad \exists N$ s.t. $$d_{S}(x_{n},p) < \delta, \qquad \textrm{ whenever } n \geq N$$
Hence, we have that
$$d_{T}(f(x_{n}),f(p)) < \epsilon \quad \textrm{ (By Definition 4.15(continuity))}$$
and therefore, by using Definition 4.1 twice
$$\lim_{n \to \infty} f(x_{n}) = f(p) $$
$$\lim_{n \to \infty} f(x_{n}) = f(\lim_{n \to \infty}x_{n}) $$
($\Longleftarrow$) Assume $\{f(x_{n})\} \to f(p)$ in $T$ and %$\{x_{n}\} \to p$ in $S$.%
Then by Definition 4.1, we have
\[d_{T}(f(x_{n}),f(p)) < \epsilon \qquad \textrm{for } \epsilon > 0\]%\qquad d_{S}(x_{n},p) < \delta\]
By Definition 4.11, we have that when $\lim_{n \to \infty}f(x_{n}) = f(p), \forall \epsilon > 0 \quad \exists \delta >0 $ such that
\newline{}
$d_{T}(f(x_{n}),f(p)) < \epsilon \quad \textrm{ whenever } d_{S}(x_{n},p) < \delta$, and hence $\delta$ is defined by $\epsilon$. Therefore we have that $\forall \epsilon > 0, \quad \exists \delta > 0$ such that
\[d_{T}(f(x),f(p)) < \epsilon \qquad \textrm{  whenever  } d_{S}(x,p) < \delta\]
and hence, $f$ is continuous at $p$.

\end{proof}

\begin{theorem}[theorem 4.23]
Let $f:S \to T$  be a function from one metric space $(S,d_{S})$ to another $(T,d_{T})$. Then $f$ is continuous on $S$ iff for every open set $Y$ in $T$, the inverse image $f^{-1}(Y)$ is open on $S$.
%Continuous Functions in Metric Spaces: Thm. 4.16, Thm. 4.23, Thm. 4.25
\end{theorem}
\begin{proof}
($\Longrightarrow$) Assume $Y$ is open and $f$ is continuous, Since $Y$ is open we have that $B_{T}(y,\epsilon) \subseteq Y$ for some $\epsilon > 0$. Let $p \in f^{-1}(Y)$ and $y=f(p)$, And Since $f$ is continuous on $S$ we have that $f$ is continuous at $p$ and hence $\exists \delta > 0$ such that  $f(B_{S}(p,\delta)) \subseteq B_{T}(y,\epsilon)$. Hence,
\[B_{S}(p,\delta) \subseteq f^{-1}[f(B_{S}(p,\delta))] \subseteq f^{-1}[B_{T}(y,\epsilon)] \subseteq f^{-1}(Y)\]
Then $p$ is an interior point of $f^{-1}(Y)$ and by Definition 3.6, if a set contains all its interior points then it is open, and since $p$ can be any point in $f^{-1}(Y)$ we have that $f^{-1}(Y)$ contains all its interior points and hence is open.
\newline{}
($\Longleftarrow$) Conversely, assume $f^{-1}(Y)$ is open in $S$ for every open subset set $Y$ in $T$. Take $p \in S$ and again let $y=f(p)$, then we want to show $f$ is continuous at $p$. $\forall \epsilon > 0$, $B_{T}(y,\epsilon)$ is open in $T$, hence $f^{-1}(B_{T}(y,\epsilon))$ is open in $S$. Since $f^{-1}(y) = p$ ,  $p \in f^{-1}(B_{T}(y,\epsilon))$, then $\exists \delta > 0$ such that,
\[B_{S}(p,\delta) \subseteq f^{-1}(B_{T}(y,\epsilon)) \Rightarrow f(B_{S}(p,\delta)) \subseteq B_{T}(y,\epsilon)\]
By definition 4.15, a function $f$ is continuous at $p$ iff $\forall \epsilon > 0, \quad \exists \delta > 0$ such that $ f(B_{S}(p,\delta)) \subseteq B_{T}(f(p),\epsilon)$, and hence we have that $f$ is continuous at $p$.
$$ $$
\end{proof}

\begin{theorem}[theorem 4.25]
Let $f:S \to T$  be a function from one metric space $(S,d_{S})$ to another $(T,d_{T})$. If $f$ is continuous on compact subset $X$ of $S$, then the image $f(X)$ is a compact subset of $T$; in particular, $f(X)$ is closed and bounded in $T$.
%Continuous Functions in Metric Spaces: Thm. 4.16, Thm. 4.23, Thm. 4.25
\end{theorem}
\begin{proof}
By Definition 3.30, a set $S$ is compact iff every open covering of $S$ contains a finite subcovering, that is, a finite subcovering that also covers $S$. And by Theorem 3.38, if $S$ is compact then it is also closed and bounded.
\newline{}
Hence our goal is to find a finite open subcovering of $f(X)$ and the assertion will be proved. Define $F$ to be an open covering of $f(X)$, so that $f(X) = \cup_{A \in F}A$. Since $f$ is continuous on $S$ and hence every subset of $S$, we can apply Theorem 4.23 to see that $\forall A \in F$, $f^{-1}(A)$ is also open in $(X,d_{S})$. Hence, $X \subseteq \cup_{A \in F}f^{-1}(A) $. Since $X$ is compact, we know by the definition of a compact set that there exists an open subcovering of $X$, hence $X \subseteq \cup_{k = 1}^{n}f^{-1}(A_{k}) $ for $A_{k} \in F$. And hence,
$$ f(X) \subseteq f(\cup_{k = 1}^{n}f^{-1}(A_{k})) = \cup_{k = 1}^{n}f(f^{-1}(A_{k})) = \cup_{k = 1}^{n}A_{k} $$
$\Rightarrow$ $f(X) \subseteq\cup_{k = 1}^{n}A_{k}$ and we have found our open subcovering, and hence $f(X)$ is closed and compact in $T$.
$$ $$
$$ $$
\end{proof}

\underline{\textbf{Problem 6}}
\begin{theorem}[Uniform Continuity Thm. 4.47 (Heine)]
Let $f:S \to T$ be a function from one metric space $(S,d_{S})$ to another $(T,d_{T})$. Let $A$ be a compact subset of $S$ and assume that $f$ is continuous on $A$. Then $f$ is uniformly continuous on $A$.
\end{theorem}
\begin{proof}
Let $\epsilon > 0$ be given. Then each point $a \in A$ has associated with it a ball  $B_{S}(a,r)$, with $r$ depending on $a$ such that,
$$d_{T}(f(x),f(a)) < \frac{\epsilon}{2} \qquad \textrm{whenever } x\in B_{S}(a,r)\cap A$$
Consider the collection of balls $B_{S}(a,r/2)$. Since $A$ is compact, a finite number of them also cover $A$, say
$$ A \subseteq \cup_{k=1}^{n} B_{S}(a_{k},\frac{r_{k}}{2})  $$
in any ball with twice the radius, $B_{S}(a_{k},r_{k})$, we have
$$d_{T}(f(x),f(a_{k})) < \frac{\epsilon}{2} \qquad \textrm{whenever } x\in B_{S}(a_{k},r_{k}) \cap A$$
Let $\delta = min(r_{n}/2)$. We want to show that $\delta$ satisfies the condition of uniform continuity. Take $x \in A$ and $p \in A$ with $d_{S}(x,p) < \delta$. it is clear that $x \in B_{S}(a_{k},r_{k}/2)$, hence
$$ d_{T}(f(x),f(a_{k})) < \frac{\epsilon}{2} $$
By the Triangle inequality and our defined $\delta$, we have
$$d_{S}(p,a_{k}) \leq d_{S}(p,x) + d_{S}(x,a_{k}) < \delta + \frac{r_{k}}{2} \leq \frac{r_{k}}{2} + \frac{r_{k}}{2} = r_{k}$$
Hence, $d_{S}(p,a_{k}) \leq r_{k}$ and thus $p \in B_{S}(a_{k},r_{k}) \cap S$. Using the Triangle inequality and the fact that $d_{T}(f(p),f(a_{k})) < \epsilon /2$, we have
$$d_{T}(f(x),f(p)) \leq d_{T}(f(x),f(a_{k})) + d_{T}(f(a_{k}), f(p)) < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon$$
hence we have that $d_{T}(f(x),f(p)) < \epsilon$ and $d_{S}(x,p) < \delta$, thus fulfilling the condition for uniform continuity.
$$ $$
$$ $$
\end{proof}

\underline{\textbf{The following 2 theorems are with respect to problem 7}}
\begin{theorem}[Thm 5.10 (Rolle)]
Let $f$ be a functions, having a derivative (finite or infinite) at each point
of an open interval $(a,b)$ and continuous at the endpoints $a$ and $b$. if $f(a)=f(b)$, then there is at least one interior point c at which
\[f^\prime(c) = 0\]
%Application: Thm 5.11
\end{theorem}
\begin{proof}
Assume $f^\prime(x) \neq 0 \quad \forall x \in (a,b)$, we want to arrive at a contradiction. Since $f$ is continuous on a compact interval $(a,b)$, $\exists c_{1},c_{2} \in (a,b)$ such that $$f(c_{1}) = min(f(x)) \quad \textrm{ and } \quad f(c_{2}) = max(f(x))$$
by Theorem 5.9, $c_{1}, c_{2}$ cannot be interior points since this implies $f^\prime(c_{1}) = 0$ and $f^\prime(c_{2}) = 0$. This implies that $f(c_{1})$ and $f(c_{2})$ are endpoints. Since $f(a) = f(b) \Rightarrow f(c_{1}) = f(c_{2})$ and hence $f$ is constant on $[a,b]$. However this contradicts our assumption that $f^\prime(x) \neq 0 \quad \forall x \in (a,b)$. Therefore $f^\prime(c) = 0$ for some $c \in (a,b)$.\newline{}
\end{proof}

\begin{theorem}[Thm 5.12 (Generalized Mean Value)]
Let $f$ and $g$ be two functions, each having a derivative (finite or infinite) at each point of an open interval $(a,b)$ and each continuous at the endpoints $a$ and $b$. Assume also that there is no interior point $x$ at which both $f^\prime(x)$ and $g^\prime(x)$ are infinite. Then for some interior point $c$ we have
\[f^\prime(c)[g(b)-g(a)] = g^\prime(c)[f(b)-f(a)]\]
%Application: Thm 5.11
\end{theorem}
\begin{proof}
Let $h(x) = f(x)[g(b) - g(a)] - g(x)[f(b) - f(a)]$. Since $h(x)$ is a linear combination of $f(x)$ and $g(x)$, $h(x)$ inherits many of the assumptions about $f(x)$ and $g(x)$, which is exactly what we want in order to use Rolle's theorem. Note that,
\begin{align*}
$h(a)$ &= $ f(a)[g(b) - g(a)] - g(a)[f(b) - f(a)]$
\\ &= $ f(a)g(b) - g(a)f(b)$
\\ &= $ - f(b)g(a) + g(b)f(a)$
\\ &= $ f(b)[g(b) - g(a)] - g(b)[f(b) - f(a)]$
\\ &= $ h(b)$
\end{align*}
So $h(a) = h(b)$, $ \Rightarrow $ by Rolle's theorem we have that $\exists $ $c$ such that $ h^\prime(c) = 0 $. And Hence,
\[h^\prime(c) = f^\prime(c)[g(b) - g(a)] - g^\prime(c)[f(b) - f(a)] = 0\]
$ \Rightarrow $
\[f^\prime(c)[g(b)-g(a)] = g^\prime(c)[f(b)-f(a)]\]
This proves the assertion.\newline{}
\end{proof}

\textbf{Applications of 5.12}
\begin{theorem}[Mean-Value Theorem(5.11)]
Assume that $f$ has a derivative (finite or infinite) at each point of $(a,b)$, and assume that $f$ is continuous at both endpoints $a$ and $b$. Then $\exists c \in (a,b)$ such that
\[f(b) - f(a) = f^\prime(c)(b-a)\]
\end{theorem}
\begin{proof}
Consider Theorem 5.12, The General Mean-Value Theorem. We have that
\[f^\prime(c)[g(b)-g(a)] = g^\prime(c)[f(b)-f(a)]\]
Letting $g(x) = x$, then if we can find $g^\prime(x)$ we could  substitute and simplify to get 
\[f(b) - f(a) = f^\prime(c)(b-a)\]
Let's first find $g^\prime(x)$
\begin{equation} \label{eq1}
\begin{split}
$g'(x)$ &= $ \lim_{x \to c} \displaystyle \frac{g(x) - g(c)}{x - c} $ \nonumber \\
&= $\lim_{x \to c} \displaystyle \frac{x - c}{x - c}$ \nonumber \\
&= 1
\end{split}
\end{equation}
Now substituting $g(x) = x$ and $g^\prime(x) = 1$ into the result of Theorem 5.12 we get,
\[f^\prime(c)[b - a] =1[f(b)-f(a)]\]
After little simplification, we get exactly what we wanted,
\[f(b)-f(a) =f^\prime(c)(b - a)\]
\end{proof}

\underline{\textbf{The next 3 theorems are with respect to problem 8.}}
\begin{theorem}[7.8]
Assume $f \in R(\alpha)$ on $[a,b]$ and assume that $\alpha$ has a continuous derivative $\alpha^\prime$ on $[a,b]$. Then the Riemann integral $\int_{a}^{b} f(x) \alpha^\prime(x)dx$ exists and we have
\[\int_{a}^{b} f(x) d\alpha(x) = \int_{a}^{b} f(x) \alpha^\prime(x)dx\]
%Riemann-Stieljes: 7.8, 7.9, 7.12 with examples
\end{theorem}
\begin{proof}
Let $g(x) = f(x)\alpha^\prime(x)$ and consider a Riemanm sum
$$S(P,g) = \sum_{k=1}^{n}g(t_{k})\Delta x_{k} = \sum_{k=1}^{n}f(t_{k})\alpha^\prime(t_{k})\Delta x_{k}$$
Using the same $P$ and $t_{k}$ we can form the following Riemann-Stieljes integral
$$S(P,f,\alpha) =  \sum_{k=1}^{n}f(t_{k})\Delta \alpha_{k}$$
From the M.V.T. we can write,
$$\Delta \alpha_{k} = \alpha^\prime(v_{k}) \Delta x_{k} \qquad \textrm{whenever } v_{k} \in (x_{k-1},x_{k}) $$
$\Longrightarrow$
$$S(P,f,\alpha)-S(P,g) =  \sum_{k=1}^{n}f(t_{k})\alpha^\prime(v_{k}) \Delta x_{k} - \sum_{k=1}^{n}f(t_{k})\alpha^\prime(t_{k})\Delta x_{k} $$
$$= \sum_{k=1}^{n}f(t_{k})[\alpha^\prime(v_{k})-\alpha^\prime(t_{k})]\Delta x_{k}$$
Since $f$ is bounded, $\left|f(x)\right| \leq M \quad \forall x \in [a,b]$, where $M > 0$. Continuity of $\alpha^\prime$ on $[a,b]$ implies uniform continuity on $[a,b]$.
$\Rightarrow \textrm{ for } \epsilon > 0, \quad \exists \delta > 0 $ such that
$$ 0 \leq \left| x - y \right| < \delta \quad \textrm{implies} \quad \left| \alpha^\prime(x)- \alpha^\prime(y) \right| < \frac{\epsilon}{2M(b-a)}$$
If we take a partition $P_{\epsilon}^{\prime}$ with norm $\|P_{\epsilon}^{\prime}\| < \delta$, then for any finer partition $P$ we will have
$$\left| \alpha^\prime(v_{k})- \alpha^\prime(t_{k}) \right| < \frac{\epsilon}{2M(b-a)} \Rightarrow \left| S(P,f,\alpha)-S(P,g) \right| < \frac{\epsilon}{2}$$
Since $f \in R(\alpha)$ on $[a,b]$, $\exists P_{\epsilon}^{\prime \prime}$ such that
$$ \left| S(P,f,\alpha)-\int_{a}^{b}fd\alpha \right| < \frac{\epsilon}{2} $$
Hence, combining the last two inequalities we have that
$$ \left| S(P,g)-\int_{a}^{b}fd\alpha \right| < \epsilon $$
This proves the assertion.\newline{}
\end{proof}

\begin{theorem}[7.9]
Given $a < c < b$. Define $\alpha$ on $[a,b]$ as follows: The values $\alpha(a)$, $\alpha(c)$, $\alpha(b)$ are arbitrary;
\[\alpha(x) = \alpha(a) \textrm{ if } a \leq x < c\]
and
\[\alpha(x) = \alpha(b) \textrm{ if } c \leq x < b\]
Let $f$ be defined on $[a,b]$ in such a way that at least one of the functions $f$ or $\alpha$ is continuous from the left at $c$ and at least one is continuous from the right at $c$. Then $f \in R(\alpha)$ on $[a,b]$ and we have
\[\int_{a}^{b} f d\alpha = f(c)[\alpha(c+) - \alpha(c-)]\]
%Riemann-Stieljes: 7.8, 7.9, 7.12 with examples
\end{theorem}
\begin{proof}
if $c \in P$, every term in the sum $S(P,f,\alpha)$ is zero except the two terms arising from the subinterval separated by $c$. To account the nonzero terms, write
$$S(P,f,\alpha) = f(t_{k-1})[\alpha(c) - \alpha(c-)] + f(t_{k})[\alpha(c+) - \alpha(c)] $$
where $t_{k-1} \leq c \leq t_{k}$. Consider subtracting by
$f(c)[\alpha(c+)- \alpha(c-)]$ on both sides of the above equation,
$$S(P,f,\alpha) - f(c)[\alpha(c+)- \alpha(c-)] = \Delta = (f(t_{k-1})-f(c))[\alpha(c) - \alpha(c-)] + (f(t_{k})-f(c))[\alpha(c+) - \alpha(c)] $$
Hence we have the following about $\Delta$
$$\left| \Delta \right| \leq \left| f(t_{k-1})-f(c) \right| \left| \alpha(c) - \alpha(c-) \right| + \left| f(t_{k})-f(c) \right| \left| \alpha(c+) - \alpha(c) \right|$$
With respect to $\left| \Delta \right|$, if $f$ is continuous at $c,\Rightarrow \quad \forall \epsilon > 0, \quad \exists \delta > 0$ such that $\| P \| > \delta$ implies
$$ \left| f(t_{k-1})-f(c) \right| < \epsilon \quad \textrm{ and } \quad \left| f(t_{k})-f(c) \right| < \epsilon$$
Substituting our inequalities back into our equation for $\left| \Delta \right|$ we get,
$$\left| \Delta \right| \leq \epsilon \left| \alpha(c) - \alpha(c-) \right| + \epsilon \left| \alpha(c+) - \alpha(c) \right|$$
This inequality holds true whether or not $f$ is continuous at $c$. By changing $f$'s continuity from the left or right, we change the above inequality. If discontinuous from both sides at $c$, then $\Delta = 0$. If discontinuous from the right at $c$, then $\alpha(c) = \alpha(c+)$ and if discontinuous from the left at $c$, then $\alpha(c) = \alpha(c-)$. Either way, the assertion is proved.\newline{}
\end{proof}

\begin{theorem}[7.12]
Every finite sum can be written as a Riemann-Stieltjes integral. In fact, given a sum $\sum_{k=1}^{n} a_{k}$, define $f$ on $[0,n]$ as follows:
\[f(x) = a_{k}  \textrm{  if  }  k-1 < x \leq k\]
for $k=1,2,...,n$, and $f(0) = 0$. Then
\[\sum_{k=1}^{n}a_{k} = \sum_{k=1}^{n}f(k) = \int_{0}^{n}f(x)d[x]\]
where $[x]$ is the greatest integer $\leq x$.
%Riemann-Stieljes: 7.8, 7.9, 7.12 with examples
\end{theorem}
\begin{proof}
The greatest integer function is a type of step-function, hence $d[x]$ is continuous from the right and jumps 1 at each integer by definition of $[x]$. The function $f$ is continuous from the left at $1,2,...,n$ by construction of $f$. Hence we meet the conditions to apply Theorem 7.11, and the sum can therefore be written as a Riemann-Stieljes integral.
\newline{}
\end{proof}
\textbf{Ch.7 Examples:} \newline{}
Let $\{a_n\}$ be a sequence of real numbers. For $x\geq 0$, define $$A(x) = \sum_{n\leq x}a_n = \sum_{n=1}^{[x]}a_n$$
Where $[x]$ is the greatest integer in $x$ and empty sums are interpreted as zero.  Let $f$ have a continuous derivative in $[1,a]$. Use Stieltjes integrals to derive: $$\sum_{n\leq a}a_nf(n) = -\int_{1}^{a}A(x)f'(x)dx + A(a)f(a)$$
\begin{proof} Since $$\int_{1}^{a}A(x)f'(x)dx=\int_{1}^{a}A(x)df(x) \textrm{ since f has a continuous derivative on } [1, a]$$
$$= - \int_{1}^{a}f(x)dA(x)+A(a)f(a)-A(1)f(1) \textrm{ From integration by parts }$$
$$= - \sum_{n\leq a}a_nf(n)+A(a)f(a) \textrm{ by } \int_{1}^{a}f(x)dA(x)=\sum_{n=2}^{[a]}a_nf(n) \textrm{ and } A(1)=a_1$$
Therefore we know that: $$\sum_{n\leq a}a_nf(n) = -\int_{1}^{a}A(x)f'(x)dx + A(a)f(a)$$
\end{proof}

\underline{\textbf{The following 3 theorems are with respect to problem 9}}
\begin{theorem}[7.30 First Mean-Value Theorem for Riemann-Stieltjes integrals]
Assume that $\alpha$ is increasing and let $f \in R(\alpha)$ on $[a,b]$. Let $M$ and $m$ denote, respectively, the $sup$ and $inf$ of the set $\{f(x):x \in [a,b]\}$. Then $\exists c \in \mathbb{R}$ satisfying $m \leq c \leq M$ such that
\[\int_{a}^{b}f(x)d\alpha(x) = c\int_{a}^{b}d\alpha(x) = c[\alpha(b)-\alpha(a)]\]
In particular, if $f$ is continuous on $[a,b]$, then $c=f(x_{0})$ for some $x_{0} \in [a,b]$.
%Theorem 7.30, 7.32 and 7.34 with examples
\end{theorem}
\begin{proof}
if $\alpha(a) = \alpha(b) \Rightarrow \int_{a}^{b}f(x)d\alpha(x) = c[\alpha(b)-\alpha(a)] = 0$ and the condition holds trivially. Hence, assume $\alpha(a) < \alpha(b)$. Consider the following inequality that holds true for all upper and lower sums
\[m[\alpha(b) - \alpha(a)] \leq L(P,f,\alpha) \leq U(P,f,\alpha) \leq M[\alpha(b) - \alpha(a)]\]
$\int_{a}^{b}f(x)d\alpha$ must be lie in this interval. This is clear since if $m$ is the smallest $f(x)$, then clearly, the smallest "area" possible is m multiplied by the length of the integrand and similarly $M$ multiplied by the length of the integrand will be the largest possible area of the integral. Therefore,
\[m[\alpha(b) - \alpha(a)] \leq \int_{a}^{b}f(x)d\alpha \leq M[\alpha(b) - \alpha(a)]\]
\[m \leq \frac{\int_{a}^{b}f(x)d\alpha}{[\alpha(b) - \alpha(a)]} \leq M\]
\[m \leq \frac{\int_{a}^{b}f(x)d\alpha}{\int_{a}^{b}d\alpha} \leq M\]
Hence, define
$$c = \frac{\int_{a}^{b}f(x)d\alpha}{\int_{a}^{b}d\alpha}$$
Clearly, if $f$ is continuous on $[a,b]$, then we can apply the intermediate value to find that $\exists x_{0} \in [a,b] \textrm{ such that } f(x_{0}) = c$
\newline{}
\end{proof}


\begin{theorem}[7.32]
Let $\alpha$ be of bounded variation on $[a,b]$ and assume that
$f \in R(\alpha)$ on $[a,b]$. Define F by
\[F(x)=\int_{a}^{x} f d \alpha \quad \textrm{ if } x \in [a,b]\]
\begin{enumerate}[\upshape a)]
  \item $F$ is of bounded variation $[a,b]$.
  \item Every point of continuity of $\alpha$ is also a point of continuity of $F$.
  \item if $\alpha$ is increasing on $[a,b]$, then $\exists F^\prime(x)$ at each point $x \in [a,b]$ where $\alpha^\prime(x)$ exists and where $f$ is continuous. For such $x$, we have $$F^\prime(x) = f(x)\alpha^\prime(x)$$
\end{enumerate}
%Theorem 7.30, 7.32 and 7.34 with examples
\end{theorem}
\begin{proof}
\begin{enumerate}[\upshape a)]
\item if $x \ne y$ and $\alpha$ increasing on $[a,b]$, then consider the following
\begin{equation}
  \begin{split}
   F(y) - F(x)
   &= \int_{a}^{x} f d \alpha - \int_{a}^{y} f d \alpha \nonumber\\
   &= \int_{x}^{y} f d \alpha \qquad \textrm{(by Theorem 7.4, } \int_{a}^{c} f d \alpha + \int_{c}^{b} f d \alpha = \int_{a}^{b} f d \alpha) \nonumber \\
   &= c[\alpha(y) - \alpha(x)] \qquad \textrm{(by Theorem 7.30)}\\
   F(y) - F(x) &=  c[\alpha(y) - \alpha(x)] \qquad \textrm{(1)}\\
  \end{split}
 \end{equation}
where $m \leq c \leq M$. if $\alpha$ is of bounded variation on $[a,b]$, then
$$\sum_{k=1}^{n} \left| \Delta \alpha_{k} \right| \leq T$$
where $T$ is a positive number. Then consider
\begin{equation}
  \begin{split}
  \sum_{k=1}^{n} \left| \Delta F_{k} \right|
  &= \sum_{k=1}^{n} \left| F(x_{k})-F(x_{k-1}) \right| \nonumber\\
  &= \sum_{k=1}^{n} \left| c_{k}[\alpha(x_{k}) - \alpha(x_{k-1})] \right| \quad \textrm{by equation (1)} \\
  &= \sum_{k=1}^{n} \left| c_{k}\Delta \alpha_{k} \right| \\
  &\leq T\sum_{k=1}^{n} \left| c_{k} \right| \\
  \end{split}
\end{equation}
Hence $\sum_{k=1}^{n} \left| \Delta F_{k} \right|$ is bounded by a positive number $T\sum_{k=1}^{n} \left| c_{k} \right|$, therefore $F$ is of bounded variation on $[a,b]$.
\item By Theorem 7.30, we have that $F(x)=\int_{a}^{x} f d \alpha =  c[\alpha(x) - \alpha(a)]$. Hence, if $\alpha$ is continuous at a point $x$, then $F$ is also continuous at $x$.
\item Consider $F(y) - F(x) =  c[\alpha(y) - \alpha(x)]$, then by dividing $y-x$ on both sides, we have
$$\frac{F(y) - F(x)}{y-x} = c\frac{\alpha(y) - \alpha(x)}{y-x} $$
taking the limit as $y \to x$ we have,
$$\lim_{y \to x}\frac{F(y) - F(x)}{y-x}=\lim_{y \to x}c\frac{\alpha(y) - \alpha(x)}{y-x}$$
$$ F^\prime(x) = c \alpha^\prime(x) $$
Since $c = f(x_{0})$ for some $x_{0} \in [x,y]$, we have that when we take the limit as $y \to x$, then $x_{0} \in [x,x]$. Hence $x_{0} = x$, and $c = f(x)$. Therefore
$$ F^\prime(x) = f(x) \alpha^\prime(x) $$
\end{enumerate}
\end{proof}

\begin{theorem}[7.34 Second fundamental theorem of integral calculus]
Assume that $f \in R$ on $[a,b]$. Let $g$ be a function defined on $[a,b]$ such that $\exists g^\prime \in (a,b)$ and has the value
\[g^\prime(x) = f(x) \textrm{ for every } x \in (a,b)\]
At the endpoints assume that $g(a+)$ and $g(b-)$ exist and satisfy
\[g(a) - g(a+) = g(b) - g(b-)\]
Then we have
\[\int_{a}^{b} f(x)dx = \int_{a}^{b} g^\prime(x)dx = g(b) - g(a)\]
%Theorem 7.30, 7.32 and 7.34 with examples
\end{theorem}
\begin{proof}
For every partition on $[a,b]$, we can rewrite $g(b)-g(a)$ by
$$g(b)-g(a) = \sum_{k=1}^{n}[g(x_{k}) - g(x_{k-1})]= \sum_{k=1}^{n}g^\prime(t_{k})\Delta x_{k} = \sum_{k=1}^{n}f(t_{k})\Delta x_{k}$$
where $t_{k} \in (x_{k-1},x_{k})$ determined by the Mean Value Theorem by
$g^\prime(t_{k}) = \frac{g(x_{k}) - g(x_{k-1})}{x_{k}-x_{k-1}}$.
Taking $\epsilon > 0$, the  partition can be made finer such that
$$\left| g(b) - g(a) - \int_{a}^{b} f(x)dx \right| =  \left| \sum_{k=1}^{n}f(t_{k})\Delta x_{k} - \int_{a}^{b} f(x)dx \right| < \epsilon $$
Hence,
$$ \int_{a}^{b} f(x)dx = g(b) - g(a)$$
This proves the assertion.
\newline{}
\end{proof}

\textbf{Ch.7 Examples}\newline{}
\textbf{Example 1:}
The Second Mean-Value Theorem for Riemann Integrals(Thm.7.37) states that, if $f$ and $g$ be function defined on $[a,b]$ with $f$ increasing and $g$ continuous, then
$$\int_{a}^{b}f(x)g(x) dx = f(a)\int_{a}^{x_{0}}g(x)dx + f(b)\int_{x_{0}}^{b}g(x)dx $$
for some $x_{0} \in [a,b]$.
\begin{proof}
If $G(x) = \int_{a}^{x}g(x)dx$ and
$$\int_{a}^{b}f(x)g(x) dx =\int_{a}^{b}f(x)dG(x) = f(a)(G(x_{0})-G(a)) + f(b)(G(b) - G(x_{0}))$$
for some $x_{0} \in [a,b]$ by the Theorem 7.30, the First Mean-Value Theorem.
\end{proof}
\textbf{Example 2:}
Let $\alpha$ be a continuous function of bounded variation on $[a,b]$.  Assume $g\in R(\alpha)$ on $[a,b]$ and define $\beta (x)= \int_{a}^{x}g(t)d\alpha (t)$ if $x\in [a,b]$.  Show that: $\newline$
If $f$ only increases on $[a,b]$, then $\exists x_0 \in [a,b]$ such that:
$$\int_{a}^{b}fd\beta = f(a)\int_{a}^{x_0}gd\alpha + f(b)\int_{x_0}^{b}gd\alpha$$
\begin{proof}
 Since $\alpha$ is a continuous function of bounded variation on $[a, b]$, and $g\in R(\alpha)$ on $[a, b]$, we know that $\beta (x)$ is a continuous function of bounded variation on $[a, b]$, by
Theorem 7.32. Hence, by Second Mean-Value Theorem for Riemann-Stieltjes integrals, we know that: $$\int_{a}^{b}fd\beta =f(a)\int_{a}^{x_0}\beta (x)+ f(b)\int_{x_0}^{b}d\beta (x)$$
which implies that, By Theorem 7.26,
$$\int_{a}^{b}fd\beta = f(a)\int_{a}^{x_0}gd\alpha + f(b)\int_{x_0}^{b}gd\alpha$$\newline{}
$$ $$
\end{proof}
\newpage{}

\underline{\textbf{The next 4 theorems are with respect to problem 10.}}
\begin{theorem}[8.22]
If $|x| < 1$, the series $1 + x + x^2 + ... $  converges and has the sum $\frac{1}{1-x}$. If $|x| \geq 1$, the series diverges.
%Convergence Test: Thms 8.22, 8.23,8.25,8.26 with Examples
\end{theorem}
\begin{proof}
the series can be written in summation form by $\sum_{k=0}^{n}x^k$, and by multiplying it by $1-x$, we can simplify our sum to be in terms of only $x$ and $n$,
    $$(1-x)\sum_{k=0}^{n}x^k = \sum_{k=0}^{n}x^k - x\sum_{k=0}^{n}x^k$$
    $$\qquad = \sum_{k=0}^{n}(x^k - x^{k+1})$$
    $$ = 1-x^{n+1}$$
    dividing $(1-x)$ on both sides, we get
    $$\qquad \sum_{k=0}^{n}x^k = \frac{1-x^{n+1}}{(1-x)}$$
if $ \left| x \right| < 1$ and as $n \rightarrow \infty$ we have that
$$lim_{n \rightarrow \infty}x^{n+1} = 0$$
and hence
$$lim_{n \rightarrow \infty}\frac{1-x^{n+1}}{(1-x)}=\frac{1}{(1-x)}$$
if $ \left| x \right| \geq 1 \quad \Rightarrow$ as $n \rightarrow \infty$, we have that
$lim_{n \rightarrow \infty}x^{n+1}$ diverges and hence $\frac{1-x^{n+1}}{(1-x)}$ diverges.
\newline{}
(NOTE: This Theorem can be proved briefly using Theorem 8.10 on the convergence of telescopic series, letting $a_{n} = x^{k} - x^{k+1}$)
\end{proof}

\begin{theorem}[8.23 integral test]
Let $f$ be a positive decreasing function defined on $[1, \infty)$ such that $\lim_{x \to +\infty}f(x) = 0$. For $n = 1,2,...$, define
\[s_{n} = \sum_{k=1}^{n}f(k), \qquad  t_{n} = \int_{1}^{n}f(x)dx,  \qquad  d_{n}=s_{n}-t_{n}\]
Then we have:
\begin{enumerate}[\upshape a)]
  \item $0 < f(n+1) \leq d_{n+1} \leq d_{n} \leq f(1)$, \qquad for  $n=1,2,...$
  \item $\lim_{n \to \infty}d_{n}$ exists
  \item $\sum_{n=1}^{\infty}f(n)$ converges iff the sequence ${t_{n}}$ converges
  \item $0 \leq d_{k}-\lim_{n \to \infty}d_{n} \leq f(k)$, \qquad for $k=1,2,...$
\end{enumerate}
%Convergence Test: Thms 8.22, 8.23,8.25,8.26 with Examples
\end{theorem}
\begin{proof}
\newline{}
\begin{enumerate}[\upshape a)]
    \item $$t_{n+1} = \int_{1}^{n+1}f(x)dx = \sum_{k=1}^{n}\int_{k}^{k+1}f(x)dx \leq \sum_{k=1}^{n}\int_{k}^{k+1}f(k)dx = \sum_{k=1}^{n}f(k) = s_n$$
Which implies that $f(n+1) = s_{n+1}-s_n \leq s_{n+1}-t_{n+1} = d_{n+1}$ $\Rightarrow$ $0<f(n+1)\leq d_{n+1}$.
But we also have: $$d_n-d_{n+1} = t_{n+1} - t_n - (s_{n+1}-s_n) = \int_{n}^{n+1}f(x)dx - f(n+1) \geq \int_{n}^{n+1}f(n+1)dx - f(n+1)=0$$
Therefore $d_{n+1}\leq d_n \leq d_1 = f(1)$.  $\checkmark$
    \item In a), we proved that $0<f(n+1)\leq d_{n+1}\leq d_n\leq f(1)$ so $\lim_{n\to \infty}d_n$ must exist since the sequence is always decreasing but has a lower bound. $\checkmark$
    \item Since $b$ is true, we know that since $d_n$ converges then $s_n$ and $t_n$ converge as well, so $\sum_{n=1}^{\infty}f(n)$ converges whenever $\int_{1}^{\infty}f(x)dx$ converges. $\checkmark$
    \item From above we can write:
$$0\leq d_n-d_{n+1}\leq \int_{n}^{n+1}f(n)dx-f(n+1)=f(n)-f(n+1)$$
$$\textrm{and summing on n: } 0\leq \sum_{n=k}^{\infty}(d_n-d_{n+1})\leq \sum_{n=k}^{\infty}(f(n)-f(n+1)) \quad \textrm{if } k\geq 1$$
Each part of that inequality telescopes and breaks down into: $0\leq d_k-\lim_{n\to \infty}d_n\leq f(k)$ $\checkmark$
\end{enumerate}
\end{proof}

\begin{theorem}[8.25 ratio test]
Given a series $\sum a_{n}$ of nonzero complex terms, let
\[r = \lim_{n \to \infty} inf \left| \frac{a_{n+1}}{a_{n}}\right|, \qquad R = \lim_{n \to \infty} sup \left| \frac{a_{n+1}}{a_{n}}\right|\]
\begin{enumerate}[\upshape a)]
  \item The series $\sum a_{n}$ converges absolutely if $R < 1$.
  \item The series $\sum a_{n}$ diverges if $r>1$.
  \item The test is inconclusive if $r \leq 1 \leq R$.
\end{enumerate}
%Convergence Test: Thms 8.22, 8.23,8.25,8.26 with Examples
\end{theorem}
\begin{proof}
Assume that $R<1$ and take $x$ such that $R<x<1$. Since $R<1$, we know that $a_{n+1}<a_n$. This implies $\exists N$ such that $|\frac{a_{n+1}}{a_n}|<x$ whenever $n\geq N$. Since $x = \frac{x^{n+1}}{x^n}$: $$\frac{|a_{n+1}|}{x^{n+1}}<\frac{|a_n|}{x^n}\leq \frac{|a_N|}{x^N} \qquad \textrm{if }n\geq N$$
$\Rightarrow \quad |a_n|\leq cx^n$ if $n\leq N$, where $c=|a_N|x^{-N}$.  We now have the requirements fulfilled to apply the comparison test $\Rightarrow \quad \sum a_n$ converges and $a)$ is confirmed.
Now we assume that $r>1$.  $\Rightarrow \quad |a_{n+1}|>a_n \quad \forall n\geq N$ for some $N$ and therefore $\lim_{n\to \infty} \neq 0$ so $\sum a_n$ diverges and $b)$ is confirmed.
Finally, to confirm $c)$ consider the two summations: $$\sum n^{-1} \quad \sum n^{-2}$$  In both summations, $r=R=1$ but we know that $\sum n^{-1}$ diverges but that $\sum n^{-2}$ converges so the test is inconclusive confirming $c)$.
\end{proof}

\begin{theorem}[8.26 root test]
Given a series $\sum a_{n}$ of complex terms, let
\[p = \lim_{n \to \infty} sup \sqrt[n]{\mid a_{n} \mid}\]
\begin{enumerate}[\upshape a)]
  \item The series $\sum a_{n}$ converges absolutely if $p < 1$.
  \item The series $\sum a_{n}$ diverges if $p>1$.
  \item The test is inconclusive if $p=1$.
\end{enumerate}
%Convergence Test: Thms 8.22, 8.23,8.25,8.26 with Examples
\end{theorem}
\begin{proof}
Assume that $\rho <1$ and take $x$ such that $\rho <x<1$.  Since $\rho <1$ we know that $\exists N$ such that $|a_n|<x^n$ whenever $n\geq N$. We now have the elements required to run the comparison test and since $x^n$ converges to $0$ and $|a_n|<x^n$, we know that $\sum a_n$ also converges confirming $a)$. $\newline$
Now assume that $\rho >1$.  This implies that $|a_n|>1 \quad \forall a_n$ so $\lim_{n\to \infty}a_n\neq 0$ and therefore $\sum a_n$ diverges confirming $b)$.  $\newline$
Consider the two summations: $$\sum n^{-1} \quad \sum n^{-2}$$  In both summations, $\rho =1$ but we know that $\sum n^{-1}$ diverges but that $\sum n^{-2}$ converges so the test is inconclusive confirming $c)$.
\end{proof}
\textbf{Ch.8 Examples}: $\newline$
Test for convergence: $\sum_{n=1}^{\infty}n^3e^{-n}$ $\newline$
By Root Test, $\lim_{n\to \infty}\sup (\frac{n^3}{e^n})^{1/n} = \frac{1}{e}<1 \quad \Rightarrow$  The sum converges. $\newline$
Test for convergence: $\sum_{n=3}^{\infty}\frac{1}{n log n (log log n)^p}$ $\newline$
By Integral Test.  The function $f(x)=\frac{1}{n log n (log log n)^p}$ is positive, decreasing, and continuous on $[a,\infty)$ so the test is usable.  Consider: $$\int_a^\infty \frac{dx}{x log x (log log x)^p} = \int_{log log a}^\infty \frac{dy}{y^p}$$
Which implies that the series converges if $p>1$ and it diverges if $p\leq 1$

%The next 2 theorems are with respect to problem 12
%\begin{theorem}[9.5 Cauchy condition for uniform convergence of series]
%The infinite series $\sum f_{n}(x)$ converges uniformly on $S$ iff $\forall\epsilon > 0$ there is an $N$ such that $n>N$ implies,
%\[\left| \sum_{k=n+1}^{n+p} f_{k}(x) \right| < \epsilon, \qquad \textrm{ for each } p = 1,2,... \textrm{ and } \forall x \in S\]
%Thm. 9.5 and Thm 9.6 with examples.
%\end{theorem}
%\begin{proof}
%\end{proof}

%\begin{theorem}[9.6 Weierstrass M-test]
%Let $\{M_{n}\}$ be a sequence of nonnegative numbers such that
%\[0 \leq \left| f_{n}(x) \right| \leq \{M_{n}\}, \qquad \textrm{ for } n = 1,2,... \textrm{ and } \forall x \in S\]
%Then $\sum f_{n}(x)$ converges uniformly on $S$ if $\sum \{M_{n}\}$ converges.
%Thm. 9.5 and Thm 9.6 with examples.
%\end{theorem}
%\begin{proof}
%\end{proof}

\end{document}
